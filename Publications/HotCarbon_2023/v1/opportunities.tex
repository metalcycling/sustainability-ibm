\section{Opportunities}
\label{opportunities}
NOTES \\
%- We need traceability end to end \\
\textbf{Reliable end-to-end quantification:} A fundamental requirement would be to establish a reliable system to collect, store and report data concerning the power/energy utilization across the complete lifecycle of the AI model. Such a challenge poses significant difficulties as certain facilities may lack the capacity for monitoring or measuring. Furthermore, even in facilities equipped with monitoring capabilities, the methodologies employed for measuring power/energy could vary widely, such as how to allocate shared resources for model training. Consequently, this issue can be bifurcated into two principal aspects: standardization and reliable information tracking.

First and foremost, it is imperative to standardize the procedures employed for measuring, collecting, and reporting power/energy/carbon consumption data pertaining to the AI model. This measure would enable consistent and comparable reporting metrics. The achievement of this objective would necessitate extensive consultations with various stakeholders, including AI engineers, data center facility managers and operators, and experts in Green House Gas Protocol.

Secondly, it is crucial to ensure the accuracy and integrity of the recorded performance and energy consumption data during the quantification process. This issue can be addressed through the implementation of either a centralized or distributed system. In this context, blockchain-based solutions are particularly well-suited to our requirements, given their distributed nature and the immutability of the data they contain.

%- per use case optimization (in context) \\
\textbf{Use-case Optimization:} The first advantage includes cognitive shifts that would occur in the AI model design process. Historically, AI models have been developed with an emphasis on performance, to the exclusion of energy efficiency. However, upon considering our AI lifetime observability matrix, designers would begin to prioritize both performance and energy efficiency in their AI model architectures, as evidenced by factors such as neural network depth, width, input resolution, and parameters [CITE, Compute and Energy Consumption Trends in Deep Learning Inference]. This would facilitate the development of AI models that are not only high-performing but also energy-efficient, thereby reducing the need for unnecessary training practices.

Furthermore, this approach would allow for the identification of inefficiencies within the various phases of AI, such as data processing, training, and inferencing, which could be targeted for optimization. Through the implementation of scheduling techniques and power knob configurations (such as power capping and dynamic voltage and frequency scaling), we would be able to enhance the utilization of each resource while simultaneously reducing energy consumption. This would be achieved through a comprehensive understanding of the underlying energy-performance trade-offs that exist in different phases of the AI model lifecycle.

- Making decisions per persona \\

- quantifying the benefits of optimization techniques - for example is NAS good or bad? well it depends \\ 



%%\ek{we would be able to address the traceability aspect by using block chain technology}
