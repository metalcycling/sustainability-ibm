\section{Towards Metrics and Methodology for Sustainable AI}
{
    \label{sec:metrics}

    As we stated above, our approach includes defining a new concept, which we term \textit{Embodied Product Cost} (Section~\ref{extensions}), then using it to expend the metric definition from~\cite{Gandhi2022} (Section~\ref{extensions}), and lastly, applying the result to the domain of AI (Section~\ref{application}).

    \subsection{Overview of Data Center Sustainability Metric}
    {
        \label{overview}

        In this section, we give a brief overview of two of the relevant metrics defined in~\cite{Gandhi2022}. The reader is referred to~\cite{Gandhi2022} for a complete description. All metrics defined, employ the unit of ``carbon dioxide equivalent" or gCO2e~\footnote{gCO2e stands for CO2 equivalent emissions, accounting for carbon dioxide and all the other greenhouse gases, such as methane and nitrous oxide}.

        The \textbf{Job Sustainability Cost (JSC)} is aimed at quantifying the carbon footprint associated with running a job. A \textit{job} is any unit of work, that an application performs relevant for scaling (i.e., work jobs require more resources). The Job Sustainability Cost (JSC) is calculated as the sum of energy used by the job's share of all systems participating in executing the job, and also including a `tax' for the cooling and power loss overheads. Energy is converted to carbon based on the mix of energy sources and their associated carbon. For example, consider a job $j$ that consumes $\unit[1]{kJ}$ energy executing on a host, and an additional $\unit[0.08]{kJ}$ due to cooling and power distribution losses. If the energy source mix is $80\%$ coal and $20\%$ solar, then, using carbon-intensity values (and converting kJ to kWh), we have $JSC \! \left ( j \right ) = 1.08 \times (0.8 \times 820 + 0.2 \times 48) \div 3600 \approx \unit[0.2]{gCO2e}$.  

        The \textbf{Amortized Sustainability Costs (ASC)} is aimed at further including an additional `tax' for the life cycle cost of the systems used in the computation. It is calculated by adding the job’s share (or tax) of the systems' embodied emission and other costs of all systems participating in the computation to the previously defined $JSC$. For the job $j$ from the example above, assume that it runs exclusively for $5$ minutes on a system $S$ with an expected lifetime of $3$ years, and the embodied cost of $S$  is $\unit[10,\!000]{gCO2e}$. Let $ec \! \left ( j \right )$ be the `tax' for embodied carbon. Then, $ec \! \left ( j \right ) = 10,\!000 \times \frac{5}{3\times 365 \times 24 \times 60} = 0.031$, and $ASC \! \left ( j \right ) = JSC \! \left ( j \right ) + ec \! \left ( j \right ) \approx \unit[0.231]{gCO2e}$. 

        The paper defines other useful metrics, such as {\bf Job Quality per Cost Rate (JQCR)} which we will not get into here due to space limits. 
    }

    \subsection{Proposed Extension}
    {
        \label{extensions}

        We agree with \cite{Gandhi2022} that an end-to-end data center sustainability metrics must ultimately focus on the cost of a unit of work executed, and that all overheads to the extent possible, must be added in. However, one overhead which is not accounted for in the definitions in Section~\ref{overview}, is that of a job \textit{execution context}. Most jobs today, in cloud or on-premise environments, execute in a context of a continuously running shared software platform. For example, a Serverless job on AWS executes in the context of a platform termed Lambda~\cite{Lambda}. A container in IBM Cloud, is executed in a context of a platform termed IBM Kubernetes Service (IKS)~\cite{IKS}. There are significant overheads associated with the operations and life cycle of these platforms. For example, in IKS, there are a number of servers dedicated to managing the service in every location where the service is deployed. These management servers are always running, independently of the number of jobs executing. They are used to run management software to, e.g., monitor the health of the systems, and to meter usage for billing purposes. In storage and data services, such as IBM's Cloud Object Storage (COS) ~\cite{COS}, processes wake up periodically, unprompted by any user initiated action, to perform cleanup, and tier management. Other shared services may be used across services (i.e., logging service), and the proportionate share of their use must be taken into account as well (see, ~\cite{Eilam2021}). In addition, we ought to not forget about the cost of continuous deployment (CD), and testing of the platform. 

        We use the term \textit{software product} (in short, \textit{product}) to denote any software asset, such as, a software platform that is used as-a-services to support the execution of jobs, or, an AI model used in serving inference requests, or, a datasets which are prepared and then used for the training of AI models, etc. Each software product is associated with a life cycle, including development, deployment, and use. 

        Next, we define a new metric to capture the `embodied' carbon of software products, according to the same principles that are used for calculating the embodied carbon of systems. For systems, activities include material extraction, transportation, manufacturing processes, etc, factoring-in the \textit{em entire supply chain} leading to the end product. For software, activities may vary based on the type of software product. For a platform delivered as a service they will include development and testing; for a dataset it includes data preparation and de-duplication; and, for an AI model it includes training. Each such activity consists of humans working on systems that consume energy from certain power grid, with a particular energy mix. 

        The \textbf{Embodied Product Cost (EPC)} is the upfront development cost of any given software product (up to the point of its deployment and use). It is calculated as the carbon footprint associated with all activities needed to create the product. We can for example refer to an activity of a developer, or, a tester, or a data scientist, a day, as a job $j$. The Embodied Product Cost $EPC$, is the summation over all $ASC \! \left ( j \right )$ of all the jobs over the course of the creation of the product (a period that can easily take a couple of years).  

        Next, we propose to expand the definition of $JSC$ to factor-in the platform's operational cost. To avoid confusion, we denote the expanded definition as $JSC^e$.

        The \textbf{Expanded Job Sustainability Cost} (denoted $JSC^e$) is calculated as the sum of energy (converted to carbon) of all systems participating in the computation, and a `tax' for cooling and power loses, and a `tax' for the platform overhead if a platform is used as the execution context. For example, assume that the job $j$ is a container that runs in the context of a platform such as IBM's IKS~\cite{IKS}. This platform, in a particular location such as Dallas, uses $3$ servers dedicated to management, and their associated carbon is $\unit[50]{gCO2e}$ for every minute of operation. Let's assume that the job $j$ executes for $5$ minutes, and that while it is executing, there are concurrently $9$ other jobs, of roughly equal size, executing on IKS in Dallas. Then, our job is `taxed' $\frac{50}{10} \times 5$ for its share of the platform overhead. This number is added to $JSC \! \left ( j \right )$ to derive $JSC^e \! \left ( j \right )$.  In addition, we also need to add the cost of service maintenance, and continuous development and testing. We leave this as an exercise to the reader. 

        Lastly, we expand accordingly the definition of the Amortized Sustainability Cost ($ASC$) of jobs. We claim that in addition to the embodied emission of systems participating in the execution of a job, we also have to add the embodied emission of any software product that serves in the execution.

        The \textbf{Expanded Amortized Sustainability Cost}, denoted, $ASC^{e}$, is calculated as the sum of $JSC^{e}$, and, the job’s share (or tax) of the systems' embodied emission (for all systems participating in the computation), and, the Embodied Product Cost (EPC) of the platform supporting the computation. As an example, if $j$ is a container running on IKS, in Dallas, and it runs for $5$ days, concurrently with $9$ other equally sized jobs, and lets us assume that the expected life time of the IKS service in Dallas is $6$ years, then it is taxed $EPC \! \left ( IKS \right ) \times \frac{5}{10 \times 365 \times 6}$, which is added to $ASC \! \left ( j \right )$ to derive $ASC^{e} \! \left ( j \right )$. 

        With the introduction of a new metric $EPC$ to capture the embodied cost of software products, and with the expanded definition of $JCS$ and $ASC$, we are now ready to turn our attention to the unique and fascinating life of AI models. We show how we apply these three metrics to meaningfully calculate the sustainability cost of AI inference jobs. 
    }

    \subsection{Metrics for Sustainable AI} 
    {
        \label{application}

        We are now ready to apply the concepts and definitions from Section~\ref{extensions} towards metrics for Sustainable AI. 

        There are two `products' that are of key relevance for AI. The first is a dataset. Refer to Section~\ref{life-cycle} for activities used to create a dataset. Once a dataset is ready, it can be used to train multiple different models. For a dataset $d$, we calculate the Embodied Product Cost $EPC \! \left ( d \right )$ as the sum of carbon footprint of all activities involved in preparing the dataset. If we refer to each such activity as a single job then $EPC \! \left ( d \right ) = \sum_{j} ASC^{e} \! \left ( j \right )$. 

        The second `product' that is relevant to AI, is an AI model. A model is prepared (i.e., developed, or `manufactured') via a process of experimentation, and training (see Section~\ref{life-cycle}) leveraging one or more datasets. A given model can also be used to develop another, sometimes task-specific, model, in a process called \textit{distillation} or \textit{fine-tuning}. 

        For a model $m$, $EPC \! \left ( m \right )$ is calculated as the sum of the carbon footprint associated with the development (`manufacturing') of a model, recursively. Formally, $EPC \! \left ( m \right ) = ASC^{e} \! \left ( j \right ) + w_{1} \times EPC \! \left ( m^{\prime} \right ) + w_2 \times \sum EPC \! \left ( d_{i} \right )$, where, $j$ is defined as the `job' of preparing $m$ based on either another model $m^{\prime}$ or multiple datasets $d_{1}, d_{2}, \dots$ and,  $w_{1}$ is a tax weight to factor-in the re-use of a different model $m^{\prime}$. It is $0$ if there was no model that was re-used, or proportioned, based on its share of model re-use, and finally, $w_{2}$ is the tax weight, if the model was created based on datasets $d_{1}, d_{2}, \dots$. It is $0$ if there was no dataset that was used, or proportioned, based on its share of re-use. As an example, consider the RoBERTa model (\cite{Liu2019}). It was pre-trained based on a set of datasets (Wikipedia, CC-NEWS, Stories, OpenWebText, BookCorpus), using $1024$ $\unit[32]{GB}$ NVIDIA V100 GPUs for approximately one day. Assuming that the GPUs were working at full capacity, the maximum power consumption is $\unitfrac[300]{J}{s}$. Thus, the energy consumed for pre-training is $1024 \times 300 \times 360 \times 24 = \unit[737]{kwh}$. We still have to add cooling/power-loss overheads, as well as, the `tax' for the embodied system carbon footprint of the GPUs, as well as the fraction of embodied product carbon of the datasets used $EPC \! \left ( \text{CC} - \text{NEWS} + \text{BookCorpus} + \text{Wikipedia} + \text{OpenWebText} + \text{Stories} \right )$ (and then convert to carbon based on the grid energy mix). Yet as another example, DistilBERT (\cite{Sanh2019}), was created based on BERT via distillation. It has about half the total number of parameters of BERT base and retains $95\%$ of BERT’s performances on the language understanding benchmark GLUE. To create DistilBERT based on BERT, the team (\cite{Sanh2019}) used eight \unit[16]{GB} V100 GPUs for approximately three and a half days. Again assuming GPUs were used to their max capacity ($\unitfrac[250]{J}{s}$) the energy for distillation turns out to be $\unit[14.4]{kWh}$, and we need to add a tax for the fraction of re-use of BERT based on its $EPC$, and the other components as explained above.

        Once a model is `ready' it is deployed, and used to serve inference jobs. We can say that a model is \textit{deployed} when it is available to be used to serve inference jobs. We can refer to the life cycle phase when it is serving inference jobs as \textit{the operational phase}.
        
        In addition  to serving inference jobs, the model must be kept accurate, thus, it is being continuously re-trained. The frequency of re-training, and the cost of it, varies across models, and use cases. For example, in~\cite{Wu2022}, the frequency reported for two different use-cases is daily, and weekly. 

        Let's assume that at time interval $t$, a model $m$ was used to serve $n$ inference jobs, and the carbon cost of re-training at that interval was $cf_{rt}$ (note, there may have been multiple re-training `jobs' at time interval $T$, in which case we take their sum). Then, the carbon cost of re-training $cf_{rt}$, must be split equally (or in proportion to the size, for non-equal jobs), across all jobs executing at time interval $t$, i.e., $cf_{rt} / n$ is added.

        As an example, inference jobs based on DistilBERT take $\approx 410$ seconds to complete on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz), which translate to $\approx \unit[0.015]{kWh}$ of energy. This is about $60\%$ of the cost of inference with the original BERT. This demonstrates the benefits of tolerating the additional upfront cost associated with  distillation, for downstream efficiency gains. To calculate $JCA^{e}$ we need to also include the cost of re-training which is use case specific. Adding in the cost of model re-training will encourage data scientists to practice sustainability mindset, examining, for example, the needed frequency.

        Finally, lets now discuss how we calculate $ASC^{e} \! \left ( j \right )$, where $j$ is an inference job performed against a model $m$. Here, we leverage our Embodied Product Cost metric ($EPC \! \left ( m \right )$), in order to account for the carbon associated with  the construction of a model, as well as the embodied system cost. Thus, $ASC^{e} \! \left ( j \right )$ is calculated adding in both. For a model $m$, if the lifetime expectancy of the model is $LT$, for example, $3$ years, and the execution time of an inference job is $t$, and there are $n$ parallel jobs executing at any given time, then $ASC^{e} \! \left ( j \right ) = ASC \! \left ( j \right ) + \frac{EPC \! \left ( m \right ) \times t}{LT \times n}$, where $ASC \! \left ( j \right )$ is defined as expected, adding to $JSC^{e} \! \left ( j \right )$ the embodied system emission tax, and related costs. 

        Again, this metric exemplifies the usefulness for fostering a sustainability mindset. A datasetentist will be encouraged to ask questions, such as `what is the right allocation of energy to train a model based on the expected usage, and expected life time'. 
    }
}
