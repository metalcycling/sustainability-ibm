{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40751d9",
   "metadata": {},
   "source": [
    "# Tokenizer Training\n",
    "\n",
    "We will begin by training a tokenizer on our desired dataset. The Children's Book Test (CBT) dataset is used in this example. HuggingFace provides the tokenizer, and in this case the dataset as well. We will take HuggingFace's base RoBERTa tokenizer and retrain it on CBT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6df6c2e-497c-403c-8030-b5d693b4d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow import fs\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import ray\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d08a3f",
   "metadata": {},
   "source": [
    "Large-scale datasets are often split into multiple individual files, or \"shards\". While WikiText-103 is not truly large enough to merit this treatment, we will shard the dataset manually for illustrative purposes. \n",
    "\n",
    "Shards reside in `./wiki_shards/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d7d634-ebd1-4bf9-827b-88d2b776d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cbt/CN to /home/metalcycling/Temporary/Scratch/.cache/huggingface/cbt/CN/1.1.0/73e4c9316b0d86a7addd7f80183fb971a6161fa2f8b746da034e205b7e16f78d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef71ee23d504c248915c0d9cd389b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cbt downloaded and prepared to /home/metalcycling/Temporary/Scratch/.cache/huggingface/cbt/CN/1.1.0/73e4c9316b0d86a7addd7f80183fb971a6161fa2f8b746da034e205b7e16f78d. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Fetch dataset and split into 10 shards for illustration purposes\n",
    "dataset = load_dataset(\"cbt\", name = \"CN\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d385c3a-6367-447d-b4dd-68b2ff360da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"cbt_shards/\"):\n",
    "    os.mkdir(\"cbt_shards\")\n",
    "    \n",
    "num_shards = 10\n",
    "\n",
    "for i in range(num_shards):\n",
    "    shard = dataset[(i + 0) * len(dataset) // num_shards : (i + 1) * len(dataset) // num_shards][\"sentences\"]\n",
    "    \n",
    "    with open(\"cbt_shards/cbt_shard_%d.pkl\" % (i), \"wb\") as fileptr:\n",
    "        pickle.dump(shard, fileptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b741969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an iterator over our dataset that returns batches of 64 lines at a time\n",
    "shards = os.listdir(\"./cbt_shards\")\n",
    "min_sentence_length = 10\n",
    "\n",
    "def batch_iterator():\n",
    "    for shard in shards:\n",
    "        with open(\"./cbt_shards/%s\" % (shard), \"rb\") as fileptr:\n",
    "            data = pickle.load(fileptr)\n",
    "            \n",
    "        batch = []\n",
    "        \n",
    "        for idx in range(len(data)):\n",
    "            paragraph = \" \".join(data[idx])\n",
    "        \n",
    "        for sentences in data:\n",
    "            paragraph = \n",
    "            if len(line) > min_sentence_length: # remove trivially short sentences\n",
    "                batch.append(line)\n",
    "                \n",
    "            if len(batch) == 64: # If batch is of size 64 return it\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        yield batch\n",
    "        \n",
    "        print(\"Shard '%s' completed\" % (shard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5ffcbc0-1b26-4f99-a276-76b192dfe265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12076"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb74528-6524-4f8b-ac34-a7406e2a0aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = batch_iterator()\n",
    "data = pickle.load(open(\"./cbt_shards/cbt_shard_0.pkl\", \"rb\"))\n",
    "print(len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97231f4e-0757-4eb5-b1f8-ba879106422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((\" \".join(data[4])).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b4d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get a BPE tokenizer with the right preprocessors, which we'll then retrain\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_new_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Programs/Anaconda/envs/ml/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:711\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.train_new_from_iterator\u001b[0;34m(self, text_iterator, vocab_size, length, new_special_tokens, special_tokens_map, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m trainer_class \u001b[38;5;241m=\u001b[39m MODEL_TO_TRAINER_MAPPING[tokenizer_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    710\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainer_class(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 711\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     trained_tokenizer_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tokenizer\u001b[38;5;241m.\u001b[39mto_str())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "# Create and train our tokenizer\n",
    "vocab = 50000 + 256 + 5 # 50K learned tokens, 256 base characters, 5 dummy tokens\n",
    "\n",
    "# Get a BPE tokenizer with the right preprocessors, which we'll then retrain\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") \n",
    "tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size = vocab)\n",
    "tokenizer.save_pretrained(\"roberta-tokenizer\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2c801",
   "metadata": {},
   "source": [
    "Now that training is done, let's take a look at how the tokenizer handles a real sentence, and how it splits apart words it does not recognize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1684f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a triumph. I'm making a note here: HUGE SUCCESS! It's hard to overstate my satisfaction.\n",
      "[0, 56, 21915, 325, 263, 12076, 18, 339, 11, 81, 1896, 263, 6100, 4714, 30, 27994, 33808, 27821, 8572, 7637, 55, 5, 625, 11, 87, 2647, 294, 605, 6077, 1656, 23118, 18, 2]\n",
      "<s>\n",
      "T\n",
      "his\n",
      " was\n",
      " a\n",
      " triumph\n",
      ".\n",
      " I\n",
      "'\n",
      "m\n",
      " making\n",
      " a\n",
      " note\n",
      " here\n",
      ":\n",
      " HU\n",
      "GE\n",
      " SU\n",
      "CC\n",
      "ES\n",
      "S\n",
      "!\n",
      " It\n",
      "'\n",
      "s\n",
      " hard\n",
      " to\n",
      " over\n",
      "state\n",
      " my\n",
      " satisfaction\n",
      ".\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Test run for our new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-tokenizer\")\n",
    "\n",
    "sample = \"This was a triumph. I'm making a note here: HUGE SUCCESS! It's hard to overstate my satisfaction.\"\n",
    "print(sample)\n",
    "\n",
    "tokens = tokenizer(sample)[\"input_ids\"]\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\n\".join([tokenizer.decode(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84535f29",
   "metadata": {},
   "source": [
    "# Pre-Tokenizing the Dataset\n",
    "\n",
    "Now that we have a trained tokenizer, let's convert our dataset into token indices. At the same time, we'll pack sequences together until we exceed the maximum RoBERTa sequence length of 512. At that point we back off, pad out the sequence, and write that batch to our output shards. By tokenizing, packing, and padding sequences all in advance, we can compress our dataset size, accelerate downstream dataloading, and streamline our training procedure.\n",
    "\n",
    "This time we will use Ray for parallelism - two workers will iterate over their respective input shards, and each will write a single output shard that we'll use for training. Currently we only support scaling on single nodes - do not attempt to run this notebook on OpenShift!\n",
    "\n",
    "*(Parallelism on OpenShift for both tokenizer training and preprocessing will be provided in a future release)*\n",
    "\n",
    "Output shards reside in `./wiki_processed_shards/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e5cb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workload that each parallel Ray actor will run\n",
    "def process(config):\n",
    "    shards = config[\"shards\"]\n",
    "    directory = config[\"directory\"]\n",
    "    \n",
    "    seq_len = 512\n",
    "    shard = session.get_world_rank()\n",
    "    # Distribute input shards over workers, each of which produces a single output shard\n",
    "    subsets = shards[(shard + 0) * len(shards) // session.get_world_size() : (shard + 1) * len(shards) // session.get_world_size()] \n",
    "    \n",
    "    # LOAD OUR PRETRAINED TOKENIZER\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"%s/roberta-tokenizer\" % (directory))\n",
    "    schema = pa.schema([pa.field(\"nums\", pa.int16())])\n",
    "    pad = tokenizer(\"<pad>\")[\"input_ids\"][1]\n",
    "    eos = tokenizer(\"</s>\")[\"input_ids\"][1]\n",
    "    \n",
    "    # ITERATE OVER INPUT FILES, WRITE BATCHES 512 TOKENS AT A TIME TO OUR DATASET SHARD\n",
    "    buffer = [] # our write buffer\n",
    "    counter = 0 # written batch counter\n",
    "    ntrunc = 0\n",
    "    npad = []\n",
    "    \n",
    "    with pa.ipc.new_file(\"%s/wiki_processed_shards/shard_%03d.arrow\" % (directory, session.get_world_rank()), schema) as writer:\n",
    "        for j in range(len(subsets)): # for each input shard\n",
    "            filename = subsets[j]\n",
    "            \n",
    "            with open(\"%s/wiki_shards/%s\" % (directory, filename), \"rb\") as fileptr:\n",
    "                dataset = pickle.load(fileptr)\n",
    "                \n",
    "            for entry in dataset:\n",
    "                line = tokenizer(entry)[\"input_ids\"] # tokenize!\n",
    "                \n",
    "                if len(line) > 5: # Ignore short sentences\n",
    "                    if len(line) > seq_len: # Truncate long sentences\n",
    "                        line = line[:seq_len - 1] + [eos]\n",
    "                        ntrunc += 1\n",
    "        \n",
    "                    if len(buffer) + len(line) <= seq_len: # If line fits into buffer, add it\n",
    "                        buffer += line\n",
    "                    else: \n",
    "                        # Else, pad out buffer\n",
    "                        npad.append(seq_len - len(buffer))\n",
    "                        buffer += [pad,] * (seq_len - len(buffer))\n",
    "                        \n",
    "                        # Write buffer. We subtract 25K to prevent overflow - \n",
    "                        # int16 only goes up to 32767, vocab size is >50K\n",
    "                        batch = pa.record_batch([pa.array([x - 25000 for x in buffer], pa.int16())], schema)\n",
    "                        writer.write(batch)\n",
    "                        counter += 1\n",
    "                        \n",
    "                        # Clear buffer and write line\n",
    "                        buffer = line\n",
    "                        \n",
    "            print(\"Shard %d: %d of %d complete, length = %d, avg pad = %f\" % (session.get_world_rank(), j + 1, len(subsets), counter, sum(npad) / len(npad)))\n",
    "                        \n",
    "            session.report({\"training_iteration\": j + 1})\n",
    "            \n",
    "        # Write final buffer\n",
    "        buffer += [pad,] * (seq_len - len(buffer))\n",
    "        npad.append(seq_len - len(buffer))\n",
    "        batch = pa.record_batch([pa.array([x - 25000 for x in buffer], pa.int16())], schema)\n",
    "        writer.write(batch)\n",
    "        counter += 1\n",
    "        \n",
    "    writer.close()\n",
    "    \n",
    "    print(\"Shard %d complete, final length = %d lines, with %f pads per %d sequence tokens and %d truncations\" % (session.get_world_rank(), counter, sum(npad) / len(npad), seq_len, ntrunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ec06d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 16:17:16,439\tINFO data_parallel_trainer.py:286 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-31 16:21:04</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:48.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.8/62.5 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/30.26 GiB heap, 0.0/15.13 GiB objects (0.0/1.0 accelerator_type:T2000)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_07e6f_00000</td><td>TERMINATED</td><td>192.168.0.106:48720</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         223.719</td><td style=\"text-align: right;\">  1680294061</td><td style=\"text-align: right;\">            45.7014</td><td style=\"text-align: right;\">                    5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=48720)\u001b[0m 2023-03-31 16:17:18,662\tINFO data_parallel_trainer.py:286 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m 2023-03-31 16:17:20,630\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0: 1 of 5 complete, length = 26348, avg pad = 88.277061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname                                       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_07e6f_00000</td><td style=\"text-align: right;\">            45.7014</td><td style=\"text-align: right;\">  1680294061</td><td style=\"text-align: right;\">                    5</td><td>2023-03-31_16-21-02</td><td>True  </td><td>                </td><td>8ba8dac5eb9c462ba86c774329609c2d</td><td style=\"text-align: right;\">               0</td><td>li-e847394c-35a2-11b2-a85c-c5cec5aeb50b.ibm.com</td><td style=\"text-align: right;\">                         5</td><td>192.168.0.106</td><td style=\"text-align: right;\">48720</td><td style=\"text-align: right;\">             223.719</td><td style=\"text-align: right;\">           45.1526</td><td style=\"text-align: right;\">       223.719</td><td style=\"text-align: right;\"> 1680294062</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>07e6f_00000</td><td style=\"text-align: right;\">    0.0903034</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1: 1 of 5 complete, length = 26445, avg pad = 88.567215\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0: 2 of 5 complete, length = 52918, avg pad = 88.239446\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1: 2 of 5 complete, length = 53214, avg pad = 88.476735\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0: 3 of 5 complete, length = 79738, avg pad = 88.675274\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1: 3 of 5 complete, length = 79889, avg pad = 88.533090\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0: 4 of 5 complete, length = 106106, avg pad = 88.391467\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1: 4 of 5 complete, length = 106429, avg pad = 88.696568\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0: 5 of 5 complete, length = 133232, avg pad = 88.723182\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48768)\u001b[0m Shard 0 complete, final length = 133233 lines, with 88.722516 pads per 512 sequence tokens and 478 truncations\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1: 5 of 5 complete, length = 133128, avg pad = 88.704255\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=48769)\u001b[0m Shard 1 complete, final length = 133129 lines, with 88.703588 pads per 512 sequence tokens and 436 truncations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 16:21:04,802\tINFO tune.py:762 -- Total run time: 228.36 seconds (228.24 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Run our Ray-based pre-tokenizer!\n",
    "if not os.path.exists(\"./wiki_processed_shards\"):\n",
    "    os.mkdir(\"wiki_processed_shards\")\n",
    "    \n",
    "# For illustrative purposes, we'll condense our 10 input shards into 2 output shards\n",
    "trainer = TorchTrainer(train_loop_per_worker = process, train_loop_config = { \"shards\": os.listdir(\"./wiki_shards\"), \"directory\": os.getcwd() }, scaling_config = ScalingConfig(num_workers = 2))\n",
    "trainer.fit()\n",
    "\n",
    "print('Preprocessing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f2c6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size (pickle-compressed): 520 MB\n"
     ]
    }
   ],
   "source": [
    "# Original dataset size:\n",
    "orig_size = sum([os.path.getsize(\"./wiki_shards/%s\" % (filename)) for filename in os.listdir(\"./wiki_shards\")])\n",
    "print(\"Original dataset size (pickle-compressed): %s MB\" % (orig_size >> 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dd25540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset size: 308 MB\n"
     ]
    }
   ],
   "source": [
    "# New dataset size:\n",
    "new_size = sum([os.path.getsize(\"./wiki_processed_shards/%s\" % (filename)) for filename in os.listdir(\"./wiki_processed_shards\")])\n",
    "print(\"Tokenized dataset size: %s MB\" % (new_size >> 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db354406",
   "metadata": {},
   "source": [
    "*For large datasets, we have observed a roughly 3-4x reduction in tokenized dataset size compared to raw text. \n",
    "For comparison, we've found this to be slightly better than pickle, and slightly worse than gzip.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd2068029fdb0fc43dbfaa452b008b7727050663b2ad6a860292350533117808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
